# Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=test
NEO4J_DATABASE=
NEO4J_MAX_CONN_LIFETIME=60
NEO4J_ENCRYPTED=false

# LLM Configuration (llama-server or compatible OpenAI API)
LLAMA_MODEL=GLM-4.5-Air
LLAMA_TEMPERATURE=0.3
LLAMA_API_KEY=
LLAMA_BASE_URL=http://localhost:8080/v1/chat/completions
LLAMA_TOP_P=0.95
LLAMA_MAX_TOKENS=4096
LLAMA_TIMEOUT=1200

# Alternative LLM temperature setting (fallback)
LLM_TEMPERATURE=0.3

# Embedding Configuration
EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
EMBEDDING_DEVICE=cpu
EMBEDDING_CACHE_DIR=

# Message Embedding Configuration
MESSAGE_EMBEDDING_MODEL=
MESSAGE_EMBEDDING_DEVICE=
MESSAGE_EMBEDDING_CACHE_DIR=
MESSAGE_EMBEDDING_BATCH_SIZE=32

# Memory Agent Configuration
MAX_ITERATIONS=10
MAX_FACTS=30
TOOL_TIMEOUT_SECONDS=10
REASONING_TRACE_LIMIT=20

# API Server Configuration
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=INFO
ENABLE_CORS=false
CORS_ALLOW_ORIGINS=
ENABLE_DEBUG_ENDPOINT=false

# Rate Limiting Configuration
RATE_LIMIT_REQUESTS=10
RATE_LIMIT_WINDOW=60

